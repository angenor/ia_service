<template>
  <div class="max-w-4xl mx-auto space-y-4">
    <!-- Messages Container -->
    <div class="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-4 min-h-[400px] max-h-[600px] overflow-y-auto">
      <div v-if="messages.length === 0" class="flex items-center justify-center h-full min-h-[350px]">
        <div class="text-center">
          <font-awesome-icon icon="comments" class="text-4xl text-gray-400 dark:text-gray-600 mb-4" />
          <p class="text-gray-600 dark:text-gray-400">{{ $t('llm.startConversation') }}</p>
        </div>
      </div>
      
      <div v-else class="space-y-4">
        <div v-for="(message, index) in messages" :key="message.id" :class="[
          'flex',
          message.role === 'user' ? 'justify-end' : 'justify-start'
        ]">
          <div :class="[
            'max-w-[70%] rounded-lg p-3',
            message.role === 'user' 
              ? 'bg-blue-500 text-white' 
              : 'bg-gray-100 dark:bg-gray-700 text-gray-900 dark:text-white'
          ]">
            <p class="text-sm whitespace-pre-wrap">{{ message.content }}</p>
            <p class="text-xs mt-1 opacity-70">
              {{ formatTime(message.timestamp) }}
            </p>
          </div>
        </div>
        
        <!-- Loading indicator -->
        <div v-if="isGenerating" class="flex justify-start">
          <div class="bg-gray-100 dark:bg-gray-700 rounded-lg p-3">
            <div class="flex space-x-2">
              <div class="w-2 h-2 bg-gray-500 rounded-full animate-bounce" style="animation-delay: 0ms"></div>
              <div class="w-2 h-2 bg-gray-500 rounded-full animate-bounce" style="animation-delay: 150ms"></div>
              <div class="w-2 h-2 bg-gray-500 rounded-full animate-bounce" style="animation-delay: 300ms"></div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Input Section -->
    <ServiceInputSection 
      @generate="handleGenerate"
      :isGenerating="isGenerating"
    />

    <!-- Error Message -->
    <div v-if="error" class="bg-red-100 dark:bg-red-900/20 border border-red-400 dark:border-red-600 rounded-lg p-3">
      <p class="text-sm text-red-700 dark:text-red-300">{{ error }}</p>
    </div>
  </div>
</template>

<script setup>
import { ref, computed, onMounted, nextTick, watch } from 'vue'
import { useServiceStore } from '@/stores/service'
import { useAIServicesStore } from '@/stores/aiServices' 
import { useUserStore } from '@/stores/user'
import { useI18n } from 'vue-i18n'
import ServiceInputSection from '@/components/ServiceInputSection.vue'

const { t } = useI18n()
const serviceStore = useServiceStore()
const aiServicesStore = useAIServicesStore()
const userStore = useUserStore()

// √âtat local
const messages = ref([])
const isGenerating = ref(false)
const error = ref(null)
const messagesContainer = ref(null)
const streamingMessageId = ref(null)

// Configuration OpenRouter
const OPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions'
const OPENROUTER_API_KEY = import.meta.env.VITE_OPENROUTER_API_KEY

// V√©rifier si la cl√© API est pr√©sente
if (!OPENROUTER_API_KEY) {
  console.error('OpenRouter API key is missing. Please add VITE_OPENROUTER_API_KEY to your .env file')
}

// Get selected service data
const selectedService = computed(() => {
  if (!serviceStore.selectedService) return null
  return aiServicesStore.services.find(s => s.id === serviceStore.selectedService)
})

// Utiliser l'api_endpoint du service s√©lectionn√© directement
const getOpenRouterModel = computed(() => {
  if (!selectedService.value) return 'openai/gpt-3.5-turbo'
  
  // Utiliser directement l'api_endpoint stock√© dans la base de donn√©es
  return selectedService.value.api_endpoint || 'openai/gpt-3.5-turbo'
})

// Formatage du temps
const formatTime = (timestamp) => {
  const date = new Date(timestamp)
  return date.toLocaleTimeString('fr-FR', { 
    hour: '2-digit', 
    minute: '2-digit' 
  })
}

// Scroll vers le bas
const scrollToBottom = async () => {
  await nextTick()
  if (messagesContainer.value) {
    messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight
  }
}

// Estimation du nombre de tokens (approximation)
const estimateTokens = (text) => {
  // Approximation: 1 token ‚âà 4 caract√®res
  return Math.ceil(text.length / 4)
}

// Calcul du co√ªt bas√© sur les donn√©es du service
const calculateCost = (text) => {
  if (!selectedService.value) return 1
  
  // Utiliser le co√ªt d√©fini dans la base de donn√©es
  const baseCost = selectedService.value.default_cost_points || 1
  
  // Pour les mod√®les LLM, on peut ajuster le co√ªt en fonction de la longueur
  // mais on utilise le co√ªt de base d√©fini par l'admin
  const tokens = estimateTokens(text)
  const tokenFactor = Math.max(1, tokens / 1000) // Factor bas√© sur 1000 tokens
  
  return Math.max(1, Math.ceil(baseCost * tokenFactor))
}

// Gestion de la g√©n√©ration
const handleGenerate = async (inputData) => {
  if (!inputData.mainInput || inputData.mainInput.trim() === '') return
  
  const userMessage = {
    id: Date.now(),
    role: 'user',
    content: inputData.mainInput.trim(),
    timestamp: new Date()
  }
  
  messages.value.push(userMessage)
  isGenerating.value = true
  error.value = null
  
  await scrollToBottom()
  
  try {
    // V√©rifier les points de l'utilisateur
    const estimatedCost = calculateCost(inputData.mainInput)
    if (userStore.points < estimatedCost) {
      throw new Error(t('llm.insufficientPoints'))
    }
    
    // Pr√©parer les messages pour l'API
    const apiMessages = messages.value.map(msg => ({
      role: msg.role,
      content: msg.content
    }))
    
    // V√©rifier la cl√© API avant l'appel
    if (!OPENROUTER_API_KEY) {
      throw new Error('OpenRouter API key is not configured. Please check your .env file.')
    }
    
    // Pr√©parer la requ√™te
    // V√©rifier si le mod√®le supporte le streaming (par d√©faut true)
    const supportsStreaming = selectedService.value?.supports_streaming !== false
    // Permettre aussi de d√©sactiver via la config
    const enableStreaming = supportsStreaming && selectedService.value?.config?.enable_streaming !== false
    
    const requestBody = {
      model: getOpenRouterModel.value,
      messages: apiMessages,
      temperature: selectedService.value?.config?.temperature || 0.7,
      max_tokens: selectedService.value?.config?.max_tokens || 2048, // R√©duire la limite par d√©faut
      stream: enableStreaming, // Streaming bas√© sur le support du mod√®le ET la config
      ...(selectedService.value?.config?.additional_params || {})
    }
    
    // Log de la requ√™te envoy√©e
    console.log('üöÄ === REQU√äTE ENVOY√âE AU LLM ===')
    console.log('URL:', OPENROUTER_API_URL)
    console.log('Mod√®le:', getOpenRouterModel.value)
    console.log('Service s√©lectionn√©:', selectedService.value?.name)
    console.log('API Endpoint du service:', selectedService.value?.api_endpoint)
    console.log('Support du streaming:', supportsStreaming ? '‚úÖ Oui' : '‚ùå Non')
    console.log('Streaming activ√©:', enableStreaming ? '‚úÖ Oui' : '‚ùå Non')
    console.log('Corps de la requ√™te:', JSON.stringify(requestBody, null, 2))
    
    // Appel API OpenRouter
    const response = await fetch(OPENROUTER_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
        'HTTP-Referer': window.location.origin,
        'X-Title': 'nelo AI Platform'
      },
      body: JSON.stringify(requestBody)
    })
    
    // Log du statut de la r√©ponse
    console.log('üì° === STATUT DE LA R√âPONSE ===')
    console.log('Statut HTTP:', response.status)
    console.log('Statut OK:', response.ok)
    console.log('Headers:', Object.fromEntries(response.headers.entries()))
    
    if (!response.ok) {
      // Essayer de r√©cup√©rer le contenu brut de l'erreur
      const contentType = response.headers.get('content-type')
      console.log('‚ùå === ERREUR R√âPONSE LLM ===')
      console.log('Content-Type:', contentType)
      
      let errorMessage = t('llm.apiError')
      try {
        if (contentType && contentType.includes('application/json')) {
          const errorData = await response.json()
          console.log('Erreur JSON:', errorData)
          errorMessage = errorData.error?.message || errorData.message || errorMessage
        } else {
          // Si ce n'est pas du JSON, r√©cup√©rer le texte brut
          const errorText = await response.text()
          console.log('Erreur texte brut:', errorText)
          errorMessage = `HTTP ${response.status}: ${errorText.substring(0, 200)}...`
        }
      } catch (parseError) {
        console.error('Impossible de parser l\'erreur:', parseError)
      }
      
      throw new Error(errorMessage)
    }
    
    // V√©rifier le content-type de la r√©ponse r√©ussie
    const contentType = response.headers.get('content-type')
    const useStreaming = requestBody.stream && contentType && contentType.includes('text/event-stream')
    
    let assistantMessage = {
      id: Date.now() + 1,
      role: 'assistant',
      content: '',
      timestamp: new Date()
    }
    
    if (useStreaming) {
      // G√©rer le streaming SSE
      console.log('üîÑ === MODE STREAMING ACTIV√â ===')
      messages.value.push(assistantMessage)
      streamingMessageId.value = assistantMessage.id
      
      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let buffer = ''
      let usage = null
      
      try {
        while (true) {
          const { done, value } = await reader.read()
          if (done) break
          
          buffer += decoder.decode(value, { stream: true })
          const lines = buffer.split('\n')
          buffer = lines.pop() || ''
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const jsonStr = line.slice(6)
              if (jsonStr === '[DONE]') {
                console.log('‚úÖ === STREAMING TERMIN√â ===')
                break
              }
              
              try {
                const chunk = JSON.parse(jsonStr)
                console.log('üì¶ Chunk re√ßu:', chunk)
                
                if (chunk.choices && chunk.choices[0]?.delta?.content) {
                  const newContent = chunk.choices[0].delta.content
                  console.log('üìù Nouveau contenu:', newContent)
                  
                  // Mettre √† jour le message en temps r√©el
                  const msgIndex = messages.value.findIndex(m => m.id === assistantMessage.id)
                  if (msgIndex !== -1) {
                    // Utiliser splice pour forcer la r√©activit√©
                    const updatedMessage = {
                      ...messages.value[msgIndex],
                      content: messages.value[msgIndex].content + newContent
                    }
                    messages.value.splice(msgIndex, 1, updatedMessage)
                    
                    console.log('üìÑ Contenu total actuel:', updatedMessage.content.length, 'caract√®res')
                  }
                  // Scroll vers le bas apr√®s chaque update
                  await nextTick()
                  await scrollToBottom()
                }
                
                // Capturer les donn√©es d'utilisation si disponibles
                if (chunk.usage) {
                  usage = chunk.usage
                }
              } catch (e) {
                console.warn('Impossible de parser le chunk:', jsonStr, e)
              }
            }
          }
        }
      } finally {
        reader.releaseLock()
        streamingMessageId.value = null
      }
      
      console.log('‚úÖ === R√âPONSE STREAMING COMPL√àTE ===')
      console.log('Contenu final:', assistantMessage.content)
      
      // Calculer le co√ªt avec les donn√©es d'utilisation si disponibles
      console.log('üí∞ === CALCUL DU CO√õT (STREAMING) ===')
      if (usage) {
        const actualTokens = usage.prompt_tokens + usage.completion_tokens
        const tokenFactor = Math.max(1, actualTokens / 1000)
        const actualCost = Math.max(1, Math.ceil(selectedService.value.default_cost_points * tokenFactor))
        console.log('Tokens utilis√©s:', usage)
        console.log('Co√ªt calcul√©:', actualCost)
        await userStore.deductPoints(actualCost)
      } else {
        console.log('Pas de donn√©es d\'utilisation, utilisation du co√ªt estim√©:', estimatedCost)
        await userStore.deductPoints(estimatedCost)
      }
      
    } else {
      // Mode non-streaming (JSON classique)
      if (!contentType || !contentType.includes('application/json')) {
        console.error('‚ö†Ô∏è Content-Type inattendu:', contentType)
        const rawText = await response.text()
        console.error('Contenu brut re√ßu:', rawText.substring(0, 500))
        throw new Error('La r√©ponse n\'est pas du JSON valide')
      }
      
      const data = await response.json()
      
      console.log('‚úÖ === R√âPONSE RE√áUE DU LLM ===')
      console.log('R√©ponse compl√®te:', JSON.stringify(data, null, 2))
      
      assistantMessage.content = data.choices[0].message.content
      messages.value.push(assistantMessage)
      
      // D√©duire les points bas√©s sur l'utilisation r√©elle ou le co√ªt estim√©
      console.log('üí∞ === CALCUL DU CO√õT ===')
      if (data.usage) {
        const actualTokens = data.usage.prompt_tokens + data.usage.completion_tokens
        const tokenFactor = Math.max(1, actualTokens / 1000)
        const actualCost = Math.max(1, Math.ceil(selectedService.value.default_cost_points * tokenFactor))
        console.log('Tokens utilis√©s:', data.usage)
        console.log('Co√ªt calcul√©:', actualCost)
        await userStore.deductPoints(actualCost)
      } else {
        console.log('Pas de donn√©es d\'utilisation, utilisation du co√ªt estim√©:', estimatedCost)
        await userStore.deductPoints(estimatedCost)
      }
    }
    
  } catch (err) {
    console.error('LLM Error:', err)
    error.value = err.message || t('llm.genericError')
  } finally {
    isGenerating.value = false
    await scrollToBottom()
  }
}

// R√©initialiser la conversation quand on change de service
watch(() => serviceStore.selectedService, () => {
  messages.value = []
  error.value = null
})

onMounted(async () => {
  messagesContainer.value = document.querySelector('.overflow-y-auto')
  
  // S'assurer que les services sont charg√©s
  if (aiServicesStore.services.length === 0) {
    await aiServicesStore.fetchAllData()
  }
})
</script>

<style scoped>
@keyframes bounce {
  0%, 80%, 100% {
    transform: translateY(0);
  }
  40% {
    transform: translateY(-10px);
  }
}
</style>